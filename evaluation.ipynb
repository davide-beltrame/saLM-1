{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8babb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation on medqa\n",
    "\n",
    "def test_medqa(ensemble_model, baseline_model, ensemble_tokenizer, baseline_tokenizer, dataset, output_dir, device = 'cpu'):\n",
    "    \"\"\"\n",
    "    Evaluate performance of ensemble model vs baseline model and random guessing on MedQA dataset.\n",
    "    \n",
    "    Args:\n",
    "        ensemble_model: Your custom ensemble model\n",
    "        baseline_model: The baseline huggingface model for comparison\n",
    "        dataset: MedQA formatted dataset with 'question' and 'answer' columns\n",
    "        output_dir: Directory to save results and visualizations\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing all metrics and results\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import re\n",
    "    import numpy as np\n",
    "    import random\n",
    "    from tqdm import tqdm\n",
    "    import json\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "    from collections import Counter\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Ensure models are in evaluation mode\n",
    "    ensemble_model.eval() # <------------------------------------------------------------------------- TODO\n",
    "    baseline_model.eval()\n",
    "    \n",
    "    # Helper function to extract the answer letter from generated text\n",
    "    def extract_answer(text):\n",
    "        # Look for patterns like \"The correct answer is: X)\" or just \"X)\"\n",
    "        match = re.search(r'(?:The correct answer is:?\\s*)?([A-E])\\)', text)\n",
    "        if match:\n",
    "            letter = match.group(1).upper()\n",
    "            return letter\n",
    "            \n",
    "        # If no match with the above pattern, try a simpler pattern to find any letter\n",
    "        letter_match = re.search(r'\\b([A-E])\\b', text)\n",
    "        if letter_match:\n",
    "            letter = letter_match.group(1).upper()\n",
    "            return letter\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    # Initialize metrics\n",
    "    metrics = {\n",
    "        \"total_questions\": len(dataset),\n",
    "        \"ensemble\": {\n",
    "            \"correct\": 0,\n",
    "            \"letter_found\": 0,\n",
    "            \"generated_lengths\": [],\n",
    "            \"letter_distribution\": Counter()\n",
    "        },\n",
    "        \"baseline\": {\n",
    "            \"correct\": 0,\n",
    "            \"letter_found\": 0,\n",
    "            \"generated_lengths\": [],\n",
    "            \"letter_distribution\": Counter()\n",
    "        },\n",
    "        \"random\": {\n",
    "            \"correct\": 0,\n",
    "            \"letter_distribution\": Counter()\n",
    "        },\n",
    "        \"examples\": []\n",
    "    }\n",
    "    \n",
    "    # Generation parameters optimized for multiple-choice answer generation\n",
    "    gen_params = {\n",
    "        \"max_new_tokens\": 512,         # Enough for the answer format\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"temperature\": 0.3,             # Lower temperature for more deterministic outputs\n",
    "        \"top_p\": 0.92,                  # Slightly increased for better coverage\n",
    "        \"top_k\": 40,                    # Restrict to only the most likely tokens\n",
    "        \"num_beams\": 3,                 # Use beam search for better results\n",
    "        \"no_repeat_ngram_size\": 2,      # Prevent repetition\n",
    "        \"repetition_penalty\": 1.2       # Penalize repeated tokens\n",
    "    }\n",
    "\n",
    "    example_indices = random.sample(range(len(dataset)), min(5, len(dataset)))\n",
    "    \n",
    "    # Process all test examples\n",
    "    print(f\"Testing on {len(dataset)} examples...\")\n",
    "    \n",
    "    for i, example in enumerate(tqdm(dataset, desc=\"Testing models\")):\n",
    "        # Extract the question and correct answer\n",
    "        question = example['question']\n",
    "        correct_answer = example['answer']\n",
    "        \n",
    "        # Extract correct answer letter\n",
    "        correct_letter = extract_answer(correct_answer)\n",
    "        if not correct_letter:\n",
    "            print(f\"Warning: Could not extract letter from correct answer: {correct_answer}\")\n",
    "            correct_letter = \"Unknown\"  # Fall back\n",
    "        \n",
    "        # Format prompt (just use the question as is)\n",
    "        prompt = f'I want you to reason and answer on the following question.\\n{question}\\n\\nStart by reasoning about the question, then provide the answer in the format \"The answer is X) <answer>\\n Reasoning:\"'\n",
    "        \n",
    "        # Track if this is an example to save\n",
    "        is_example = i in example_indices\n",
    "        example_data = {\"question\": question, \"correct_answer\": correct_answer} if is_example else None\n",
    "        \n",
    "        # Generate random choice - simulate random guessing baseline\n",
    "        # Assume A, B, C, D, E are possible options\n",
    "        available_options = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "        random_letter = random.choice(available_options)\n",
    "        \n",
    "        # Record random baseline metrics\n",
    "        metrics[\"random\"][\"letter_distribution\"][random_letter] += 1\n",
    "        if random_letter == correct_letter:\n",
    "            metrics[\"random\"][\"correct\"] += 1\n",
    "        \n",
    "        # Test ensemble model\n",
    "        try:\n",
    "            # Generate the answer with the ensemble model\n",
    "            ensemble_inputs = ensemble_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                ensemble_output = ensemble_model.generate( # <-------------------------------------------------- TODO\n",
    "                    **ensemble_inputs,\n",
    "                    **gen_params\n",
    "                )\n",
    "            \n",
    "            # Process output\n",
    "            ensemble_generated = ensemble_tokenizer.decode(ensemble_output[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Get the model's answer by taking the part after the original prompt\n",
    "            ensemble_answer = ensemble_generated[len(prompt):].strip()\n",
    "            \n",
    "            # Extract the letter\n",
    "            ensemble_letter = extract_answer(ensemble_answer)\n",
    "            \n",
    "            # Record metrics\n",
    "            metrics[\"ensemble\"][\"generated_lengths\"].append(len(ensemble_answer.split()))\n",
    "            \n",
    "            if ensemble_letter:\n",
    "                metrics[\"ensemble\"][\"letter_found\"] += 1\n",
    "                metrics[\"ensemble\"][\"letter_distribution\"][ensemble_letter] += 1\n",
    "                if ensemble_letter == correct_letter:\n",
    "                    metrics[\"ensemble\"][\"correct\"] += 1\n",
    "                    \n",
    "            if is_example:\n",
    "                example_data[\"ensemble_answer\"] = ensemble_answer\n",
    "                example_data[\"ensemble_letter\"] = ensemble_letter\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with ensemble model: {e}\")\n",
    "            if is_example:\n",
    "                example_data[\"ensemble_answer\"] = f\"Error: {str(e)}\"\n",
    "                example_data[\"ensemble_letter\"] = None\n",
    "        \n",
    "        # Test baseline model\n",
    "        try:\n",
    "            # Generate the answer with the baseline model\n",
    "            baseline_inputs = baseline_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                baseline_output = baseline_model.generate(\n",
    "                    **baseline_inputs,\n",
    "                    **gen_params\n",
    "                )\n",
    "            \n",
    "            # Process output\n",
    "            baseline_generated = baseline_tokenizer.decode(baseline_output[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Get the model's answer by taking the part after the original prompt\n",
    "            baseline_answer = baseline_generated[len(prompt):].strip()\n",
    "            \n",
    "            # Extract the letter\n",
    "            baseline_letter = extract_answer(baseline_answer)\n",
    "            \n",
    "            # Record metrics\n",
    "            metrics[\"baseline\"][\"generated_lengths\"].append(len(baseline_answer.split()))\n",
    "            \n",
    "            if baseline_letter:\n",
    "                metrics[\"baseline\"][\"letter_found\"] += 1\n",
    "                metrics[\"baseline\"][\"letter_distribution\"][baseline_letter] += 1\n",
    "                if baseline_letter == correct_letter:\n",
    "                    metrics[\"baseline\"][\"correct\"] += 1\n",
    "                    \n",
    "            if is_example:\n",
    "                example_data[\"baseline_answer\"] = baseline_answer\n",
    "                example_data[\"baseline_letter\"] = baseline_letter\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with baseline model: {e}\")\n",
    "            if is_example:\n",
    "                example_data[\"baseline_answer\"] = f\"Error: {str(e)}\"\n",
    "                example_data[\"baseline_letter\"] = None\n",
    "        \n",
    "        # Save the example if selected\n",
    "        if is_example:\n",
    "            example_data[\"random_letter\"] = random_letter\n",
    "            metrics[\"examples\"].append(example_data)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    for model_type in [\"ensemble\", \"baseline\"]:\n",
    "        # Calculate accuracy\n",
    "        metrics[model_type][\"accuracy\"] = metrics[model_type][\"correct\"] / metrics[\"total_questions\"]\n",
    "        metrics[model_type][\"letter_detection_rate\"] = metrics[model_type][\"letter_found\"] / metrics[\"total_questions\"]\n",
    "        \n",
    "        # Calculate average lengths\n",
    "        metrics[model_type][\"avg_generated_length\"] = np.mean(metrics[model_type][\"generated_lengths\"]) if metrics[model_type][\"generated_lengths\"] else 0\n",
    "        \n",
    "        # Convert letter distribution to dictionary for JSON serialization\n",
    "        metrics[model_type][\"letter_distribution\"] = dict(metrics[model_type][\"letter_distribution\"])\n",
    "    \n",
    "    # Calculate random baseline accuracy\n",
    "    metrics[\"random\"][\"accuracy\"] = metrics[\"random\"][\"correct\"] / metrics[\"total_questions\"]\n",
    "    metrics[\"random\"][\"letter_detection_rate\"] = 1.0  # Always detects a letter by definition\n",
    "    metrics[\"random\"][\"letter_distribution\"] = dict(metrics[\"random\"][\"letter_distribution\"])\n",
    "    \n",
    "    # Calculate improvement over random and baseline\n",
    "    metrics[\"improvement\"] = {\n",
    "        \"over_random\": metrics[\"ensemble\"][\"accuracy\"] - metrics[\"random\"][\"accuracy\"],\n",
    "        \"over_baseline\": metrics[\"ensemble\"][\"accuracy\"] - metrics[\"baseline\"][\"accuracy\"],\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TESTING RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total test questions: {metrics['total_questions']}\")\n",
    "    \n",
    "    print(\"\\nAccuracy (correct letter):\")\n",
    "    print(f\"  Ensemble model: {metrics['ensemble']['correct']} / {metrics['total_questions']} = {metrics['ensemble']['accuracy']:.2%}\")\n",
    "    print(f\"  Baseline model: {metrics['baseline']['correct']} / {metrics['total_questions']} = {metrics['baseline']['accuracy']:.2%}\")\n",
    "    print(f\"  Random baseline: {metrics['random']['correct']} / {metrics['total_questions']} = {metrics['random']['accuracy']:.2%}\")\n",
    "    print(f\"  Improvement over random: {metrics['improvement']['over_random']:.2%}\")\n",
    "    print(f\"  Improvement over baseline: {metrics['improvement']['over_baseline']:.2%}\")\n",
    "    \n",
    "    print(\"\\nLetter detection rate:\")\n",
    "    print(f\"  Ensemble model: {metrics['ensemble']['letter_found']} / {metrics['total_questions']} = {metrics['ensemble']['letter_detection_rate']:.2%}\")\n",
    "    print(f\"  Baseline model: {metrics['baseline']['letter_found']} / {metrics['total_questions']} = {metrics['baseline']['letter_detection_rate']:.2%}\")\n",
    "    print(f\"  Random baseline: 100% (by definition)\")\n",
    "    \n",
    "    print(\"\\nAverage generated length (words):\")\n",
    "    print(f\"  Ensemble model: {metrics['ensemble']['avg_generated_length']:.2f}\")\n",
    "    print(f\"  Baseline model: {metrics['baseline']['avg_generated_length']:.2f}\")\n",
    "    \n",
    "    print(\"\\nLetter distribution (ensemble model):\")\n",
    "    for letter, count in sorted(metrics['ensemble']['letter_distribution'].items()):\n",
    "        percentage = count/metrics['ensemble']['letter_found'] if metrics['ensemble']['letter_found'] > 0 else 0\n",
    "        print(f\"  {letter}: {count} ({percentage:.2%})\")\n",
    "    \n",
    "    print(\"\\nLetter distribution (baseline model):\")\n",
    "    for letter, count in sorted(metrics['baseline']['letter_distribution'].items()):\n",
    "        percentage = count/metrics['baseline']['letter_found'] if metrics['baseline']['letter_found'] > 0 else 0\n",
    "        print(f\"  {letter}: {count} ({percentage:.2%})\")\n",
    "    \n",
    "    print(\"\\nLetter distribution (random baseline):\")\n",
    "    total_random = sum(metrics['random']['letter_distribution'].values())\n",
    "    for letter, count in sorted(metrics['random']['letter_distribution'].items()):\n",
    "        percentage = count/total_random if total_random > 0 else 0\n",
    "        print(f\"  {letter}: {count} ({percentage:.2%})\")\n",
    "    \n",
    "    # Print example comparison\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE COMPARISONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, example in enumerate(metrics[\"examples\"]):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Question:\\n{example['question']}\")\n",
    "        print(f\"Correct answer: {example['correct_answer']}\")\n",
    "        print(f\"\\nEnsemble model answer:\\n{example.get('ensemble_answer', 'N/A')}\")\n",
    "        print(f\"Ensemble letter: {example.get('ensemble_letter', 'N/A')}\")\n",
    "        print(f\"\\nBaseline model answer:\\n{example.get('baseline_answer', 'N/A')}\")\n",
    "        print(f\"Baseline letter: {example.get('baseline_letter', 'N/A')}\")\n",
    "        print(f\"\\nRandom baseline letter: {example.get('random_letter', 'N/A')}\")\n",
    "        print(\"-\"*40)\n",
    "    \n",
    "    # Create and save visualizations\n",
    "    try:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Accuracy comparison\n",
    "        plt.subplot(2, 2, 1)\n",
    "        models = ['Ensemble', 'Baseline', 'Random']\n",
    "        accuracies = [metrics['ensemble']['accuracy'], metrics['baseline']['accuracy'], metrics['random']['accuracy']]\n",
    "        colors = ['green', 'blue', 'red']\n",
    "        plt.bar(models, accuracies, color=colors)\n",
    "        plt.title('Accuracy Comparison')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.ylim(0, max(max(accuracies) * 1.2, 1.0))  # Set y-limit with headroom\n",
    "        \n",
    "        # Add accuracy values on top of bars\n",
    "        for i, v in enumerate(accuracies):\n",
    "            plt.text(i, v + 0.01, f\"{v:.2%}\", ha='center', fontweight='bold')\n",
    "        \n",
    "        # Letter detection rate\n",
    "        plt.subplot(2, 2, 2)\n",
    "        detection_rates = [\n",
    "            metrics['ensemble']['letter_detection_rate'], \n",
    "            metrics['baseline']['letter_detection_rate'], \n",
    "            1.0  # Random always has a letter by definition\n",
    "        ]\n",
    "        plt.bar(models, detection_rates, color=colors)\n",
    "        plt.title('Letter Detection Rate')\n",
    "        plt.ylabel('Rate')\n",
    "        plt.ylim(0, 1.1)  # Set y-limit with some headroom\n",
    "        \n",
    "        # Letter distribution for ensemble model\n",
    "        plt.subplot(2, 2, 3)\n",
    "        ensemble_letters = sorted(metrics['ensemble']['letter_distribution'].keys())\n",
    "        ensemble_counts = [metrics['ensemble']['letter_distribution'].get(letter, 0) for letter in ensemble_letters]\n",
    "        plt.bar(ensemble_letters, ensemble_counts, color='green')\n",
    "        plt.title('Ensemble Model Letter Distribution')\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        # Letter distribution comparison (all models)\n",
    "        plt.subplot(2, 2, 4)\n",
    "        \n",
    "        # Get all unique letters across all models\n",
    "        all_letters = sorted(set(\n",
    "            list(metrics['ensemble']['letter_distribution'].keys()) +\n",
    "            list(metrics['baseline']['letter_distribution'].keys()) +\n",
    "            list(metrics['random']['letter_distribution'].keys())\n",
    "        ))\n",
    "        \n",
    "        # Width of each bar\n",
    "        bar_width = 0.25\n",
    "        \n",
    "        # Set position of bars on x axis\n",
    "        r1 = np.arange(len(all_letters))\n",
    "        r2 = [x + bar_width for x in r1]\n",
    "        r3 = [x + bar_width for x in r2]\n",
    "        \n",
    "        # Create bars\n",
    "        ensemble_counts = [metrics['ensemble']['letter_distribution'].get(letter, 0) for letter in all_letters]\n",
    "        baseline_counts = [metrics['baseline']['letter_distribution'].get(letter, 0) for letter in all_letters]\n",
    "        random_counts = [metrics['random']['letter_distribution'].get(letter, 0) for letter in all_letters]\n",
    "        \n",
    "        plt.bar(r1, ensemble_counts, width=bar_width, label='Ensemble', color='green')\n",
    "        plt.bar(r2, baseline_counts, width=bar_width, label='Baseline', color='blue')\n",
    "        plt.bar(r3, random_counts, width=bar_width, label='Random', color='red')\n",
    "        \n",
    "        plt.xlabel('Letter')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Letter Distribution Comparison')\n",
    "        plt.xticks([r + bar_width for r in range(len(all_letters))], all_letters)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save chart\n",
    "        chart_file = os.path.join(output_dir, 'performance_chart.png')\n",
    "        plt.savefig(chart_file)\n",
    "        print(f\"Chart saved to {chart_file}\")\n",
    "        \n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create visualization: {e}\")\n",
    "    \n",
    "    # Save detailed results\n",
    "    results_file = os.path.join(output_dir, 'results.json')\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"Detailed results saved to {results_file}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b33ffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gsm8k(ensemble_model, baseline_model, ensemble_tokenizer, baseline_tokenizer, dataset, output_dir, device='cpu'):\n",
    "    \"\"\"\n",
    "    Evaluate performance of ensemble model vs baseline model on GSM8K math reasoning dataset.\n",
    "    \n",
    "    Args:\n",
    "        ensemble_model: Your custom ensemble model\n",
    "        baseline_model: The baseline huggingface model for comparison\n",
    "        ensemble_tokenizer: Tokenizer for the ensemble model\n",
    "        baseline_tokenizer: Tokenizer for the baseline model\n",
    "        dataset: GSM8K dataset with 'question' and 'answer' columns\n",
    "        output_dir: Directory to save results and visualizations\n",
    "        sample_size: Optional, number of random samples to use (default: use all)\n",
    "        device: Device to run models on ('cuda', 'mps', or 'cpu')\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing all metrics and results\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import re\n",
    "    import numpy as np\n",
    "    import random\n",
    "    from tqdm import tqdm\n",
    "    import json\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "    from collections import Counter, defaultdict\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Ensure models are in evaluation mode\n",
    "    ensemble_model.eval()\n",
    "    baseline_model.eval()\n",
    "    \n",
    "    # Helper function to extract the numeric answer from generated text\n",
    "    def extract_answer(text):\n",
    "        \"\"\"\n",
    "        Returns a numeric answer from formats like:\n",
    "        - \"Therefore, the answer is: # <number>\"\n",
    "        - \"# <number>\"\n",
    "        \n",
    "        Returns:\n",
    "            float or int or None: The extracted answer (number or None if no match)\n",
    "        \"\"\"\n",
    "        # Try to match number patterns with \"Therefore, the answer is: # <number>\"\n",
    "        number_match = re.search(r'[Tt]herefore,\\s+the\\s+answer\\s+is:?\\s+#\\s*([-+]?\\d*\\.?\\d+)', text)\n",
    "        if number_match:\n",
    "            number_str = number_match.group(1)\n",
    "            # Convert to float or int as appropriate\n",
    "            return float(number_str) if '.' in number_str else int(number_str)\n",
    "        \n",
    "        # Try to match simpler number pattern \"# <number>\"\n",
    "        simple_number_match = re.search(r'#\\s*([-+]?\\d*\\.?\\d+)', text)\n",
    "        if simple_number_match:\n",
    "            number_str = simple_number_match.group(1)\n",
    "            # Convert to float or int as appropriate\n",
    "            return float(number_str) if '.' in number_str else int(number_str)\n",
    "        \n",
    "        # Try to match \"The answer is: <number>\"\n",
    "        answer_is_match = re.search(r'[Tt]he\\s+answer\\s+is:?\\s*([-+]?\\d*\\.?\\d+)', text)\n",
    "        if answer_is_match:\n",
    "            number_str = answer_is_match.group(1)\n",
    "            return float(number_str) if '.' in number_str else int(number_str)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # Initialize metrics\n",
    "    metrics = {\n",
    "        \"total_questions\": len(dataset),\n",
    "        \"ensemble\": {\n",
    "            \"correct\": 0,\n",
    "            \"answer_found\": 0,\n",
    "            \"generated_lengths\": [],\n",
    "            \"answer_distribution\": Counter()\n",
    "        },\n",
    "        \"baseline\": {\n",
    "            \"correct\": 0,\n",
    "            \"answer_found\": 0,\n",
    "            \"generated_lengths\": [],\n",
    "            \"answer_distribution\": Counter()\n",
    "        },\n",
    "        \"examples\": []\n",
    "    }\n",
    "    \n",
    "    # Generation parameters optimized for math reasoning\n",
    "    gen_params = {\n",
    "        \"max_new_tokens\": 512,          # Enough for detailed reasoning\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"temperature\": 0.3,            # Lower temperature for more deterministic outputs\n",
    "        \"top_p\": 0.92,                 # Slightly increased for better coverage\n",
    "        \"top_k\": 40,                   # Restrict to only the most likely tokens\n",
    "        \"do_sample\": True,             # Enable sampling for diversity\n",
    "        \"num_beams\": 3,                # Use beam search for better results\n",
    "        \"repetition_penalty\": 1.2      # Penalize repeated tokens\n",
    "    }\n",
    "\n",
    "    # Choose a few random examples for detailed analysis\n",
    "    example_indices = random.sample(range(len(dataset)), min(5, len(dataset)))\n",
    "    \n",
    "    # Track error cases\n",
    "    error_cases = {\n",
    "        \"ensemble\": [],\n",
    "        \"baseline\": []\n",
    "    }\n",
    "    \n",
    "    # Process all test examples\n",
    "    print(f\"Testing on {len(dataset)} examples...\")\n",
    "    \n",
    "    for i, example in enumerate(tqdm(dataset, desc=\"Testing models\")):\n",
    "        # Extract the question and correct answer\n",
    "        question = example['question']\n",
    "        correct_answer_text = example['answer']\n",
    "        \n",
    "        # Extract correct numeric answer from the reference answer\n",
    "        correct_answer = extract_answer(correct_answer_text)\n",
    "        if correct_answer is None:\n",
    "            print(f\"Warning: Could not extract numeric answer from: {correct_answer_text}\")\n",
    "            correct_answer = \"Unknown\"  # Fall back\n",
    "        \n",
    "        # Format prompt with chain-of-thought instruction\n",
    "        prompt = f'I want you to solve the following math problem step by step.\\n{question}\\n\\nStart by reasoning about the problem, then provide the numerical answer in the format \"Therefore, the answer is: # <number>\"'\n",
    "        \n",
    "        # Track if this is an example to save\n",
    "        is_example = i in example_indices\n",
    "        example_data = {\"question\": question, \"correct_answer_text\": correct_answer_text, \"correct_answer\": correct_answer} if is_example else None\n",
    "        \n",
    "        # Test ensemble model\n",
    "        try:\n",
    "            # Generate the answer with the ensemble model\n",
    "            print(f\"Testing ensemble model on example {i}...\")\n",
    "            ensemble_inputs = ensemble_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                ensemble_output = ensemble_model.generate(\n",
    "                    **ensemble_inputs,\n",
    "                    **gen_params\n",
    "                )\n",
    "            print(f\"Ensemble model output: {ensemble_output}\")\n",
    "            # Process output\n",
    "            ensemble_generated = ensemble_tokenizer.decode(ensemble_output[0], skip_special_tokens=True)\n",
    "            print(f\"Ensemble model generated: {ensemble_generated}\")\n",
    "            \n",
    "            # Get the model's answer by taking the part after the original prompt\n",
    "            ensemble_answer_text = ensemble_generated[len(prompt):].strip()\n",
    "            print(f\"Ensemble model answer text: {ensemble_answer_text}\")\n",
    "            # Extract the numeric answer\n",
    "            ensemble_answer = extract_answer(ensemble_answer_text)\n",
    "            \n",
    "            # Record metrics\n",
    "            metrics[\"ensemble\"][\"generated_lengths\"].append(len(ensemble_answer_text.split()))\n",
    "            \n",
    "            if ensemble_answer is not None:\n",
    "                metrics[\"ensemble\"][\"answer_found\"] += 1\n",
    "                metrics[\"ensemble\"][\"answer_distribution\"][str(ensemble_answer)] += 1\n",
    "                \n",
    "                # Check if the answer is correct (allow for small floating point differences)\n",
    "                if isinstance(ensemble_answer, (int, float)) and isinstance(correct_answer, (int, float)):\n",
    "                    if abs(ensemble_answer - correct_answer) < 1e-6:  # For floating point comparison\n",
    "                        metrics[\"ensemble\"][\"correct\"] += 1\n",
    "                elif ensemble_answer == correct_answer:  # Direct comparison for non-numeric\n",
    "                    metrics[\"ensemble\"][\"correct\"] += 1\n",
    "                    \n",
    "            if is_example:\n",
    "                example_data[\"ensemble_answer_text\"] = ensemble_answer_text\n",
    "                example_data[\"ensemble_answer\"] = ensemble_answer\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with ensemble model on example {i}: {e}\")\n",
    "            error_cases[\"ensemble\"].append({\"index\": i, \"question\": question, \"error\": str(e)})\n",
    "            if is_example:\n",
    "                example_data[\"ensemble_answer_text\"] = f\"Error: {str(e)}\"\n",
    "                example_data[\"ensemble_answer\"] = None\n",
    "        \n",
    "        # Test baseline model\n",
    "        try:\n",
    "            # Generate the answer with the baseline model\n",
    "            print(f\"Testing baseline model on example {i}...\")\n",
    "            baseline_inputs = baseline_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                baseline_output = baseline_model.generate(\n",
    "                    **baseline_inputs,\n",
    "                    **gen_params\n",
    "                )\n",
    "            \n",
    "            # Process output\n",
    "            baseline_generated = baseline_tokenizer.decode(baseline_output[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Get the model's answer by taking the part after the original prompt\n",
    "            baseline_answer_text = baseline_generated[len(prompt):].strip()\n",
    "            \n",
    "            # Extract the numeric answer\n",
    "            baseline_answer = extract_answer(baseline_answer_text)\n",
    "            \n",
    "            # Record metrics\n",
    "            metrics[\"baseline\"][\"generated_lengths\"].append(len(baseline_answer_text.split()))\n",
    "            \n",
    "            if baseline_answer is not None:\n",
    "                metrics[\"baseline\"][\"answer_found\"] += 1\n",
    "                metrics[\"baseline\"][\"answer_distribution\"][str(baseline_answer)] += 1\n",
    "                \n",
    "                # Check if the answer is correct (allow for small floating point differences)\n",
    "                if isinstance(baseline_answer, (int, float)) and isinstance(correct_answer, (int, float)):\n",
    "                    if abs(baseline_answer - correct_answer) < 1e-6:  # For floating point comparison\n",
    "                        metrics[\"baseline\"][\"correct\"] += 1\n",
    "                elif baseline_answer == correct_answer:  # Direct comparison for non-numeric\n",
    "                    metrics[\"baseline\"][\"correct\"] += 1\n",
    "                    \n",
    "            if is_example:\n",
    "                example_data[\"baseline_answer_text\"] = baseline_answer_text\n",
    "                example_data[\"baseline_answer\"] = baseline_answer\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with baseline model on example {i}: {e}\")\n",
    "            error_cases[\"baseline\"].append({\"index\": i, \"question\": question, \"error\": str(e)})\n",
    "            if is_example:\n",
    "                example_data[\"baseline_answer_text\"] = f\"Error: {str(e)}\"\n",
    "                example_data[\"baseline_answer\"] = None\n",
    "        \n",
    "        # Save the example if selected\n",
    "        if is_example:\n",
    "            metrics[\"examples\"].append(example_data)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    for model_type in [\"ensemble\", \"baseline\"]:\n",
    "        # Calculate accuracy\n",
    "        metrics[model_type][\"accuracy\"] = metrics[model_type][\"correct\"] / metrics[\"total_questions\"]\n",
    "        metrics[model_type][\"answer_detection_rate\"] = metrics[model_type][\"answer_found\"] / metrics[\"total_questions\"]\n",
    "        \n",
    "        # Calculate average lengths\n",
    "        metrics[model_type][\"avg_generated_length\"] = np.mean(metrics[model_type][\"generated_lengths\"]) if metrics[model_type][\"generated_lengths\"] else 0\n",
    "        \n",
    "        # Convert answer distribution to dictionary for JSON serialization\n",
    "        # Sort by frequency for better visualization\n",
    "        sorted_dist = dict(sorted(metrics[model_type][\"answer_distribution\"].items(), \n",
    "                                  key=lambda x: x[1], reverse=True)[:20])  # Keep top 20 most frequent\n",
    "        metrics[model_type][\"answer_distribution\"] = sorted_dist\n",
    "    \n",
    "    # Calculate improvement over baseline\n",
    "    metrics[\"improvement\"] = {\n",
    "        \"over_baseline\": metrics[\"ensemble\"][\"accuracy\"] - metrics[\"baseline\"][\"accuracy\"],\n",
    "    }\n",
    "    \n",
    "    # Add error statistics\n",
    "    metrics[\"errors\"] = {\n",
    "        \"ensemble\": len(error_cases[\"ensemble\"]),\n",
    "        \"baseline\": len(error_cases[\"baseline\"])\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TESTING RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total test questions: {metrics['total_questions']}\")\n",
    "    \n",
    "    print(\"\\nAccuracy (correct numeric answer):\")\n",
    "    print(f\"  Ensemble model: {metrics['ensemble']['correct']} / {metrics['total_questions']} = {metrics['ensemble']['accuracy']:.2%}\")\n",
    "    print(f\"  Baseline model: {metrics['baseline']['correct']} / {metrics['total_questions']} = {metrics['baseline']['accuracy']:.2%}\")\n",
    "    print(f\"  Improvement over baseline: {metrics['improvement']['over_baseline']:.2%}\")\n",
    "    \n",
    "    print(\"\\nAnswer detection rate:\")\n",
    "    print(f\"  Ensemble model: {metrics['ensemble']['answer_found']} / {metrics['total_questions']} = {metrics['ensemble']['answer_detection_rate']:.2%}\")\n",
    "    print(f\"  Baseline model: {metrics['baseline']['answer_found']} / {metrics['total_questions']} = {metrics['baseline']['answer_detection_rate']:.2%}\")\n",
    "    \n",
    "    print(\"\\nAverage generated length (words):\")\n",
    "    print(f\"  Ensemble model: {metrics['ensemble']['avg_generated_length']:.2f}\")\n",
    "    print(f\"  Baseline model: {metrics['baseline']['avg_generated_length']:.2f}\")\n",
    "    \n",
    "    print(\"\\nErrors:\")\n",
    "    print(f\"  Ensemble model: {metrics['errors']['ensemble']} errors\")\n",
    "    print(f\"  Baseline model: {metrics['errors']['baseline']} errors\")\n",
    "    \n",
    "    print(\"\\nTop 5 most common answers (ensemble model):\")\n",
    "    for i, (answer, count) in enumerate(list(metrics['ensemble']['answer_distribution'].items())[:5]):\n",
    "        percentage = count/metrics['ensemble']['answer_found'] if metrics['ensemble']['answer_found'] > 0 else 0\n",
    "        print(f\"  {answer}: {count} ({percentage:.2%})\")\n",
    "    \n",
    "    print(\"\\nTop 5 most common answers (baseline model):\")\n",
    "    for i, (answer, count) in enumerate(list(metrics['baseline']['answer_distribution'].items())[:5]):\n",
    "        percentage = count/metrics['baseline']['answer_found'] if metrics['baseline']['answer_found'] > 0 else 0\n",
    "        print(f\"  {answer}: {count} ({percentage:.2%})\")\n",
    "    \n",
    "    # Print example comparison\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE COMPARISONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, example in enumerate(metrics[\"examples\"]):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Question:\\n{example['question']}\")\n",
    "        print(f\"Correct answer text: {example['correct_answer_text']}\")\n",
    "        print(f\"Correct numeric answer: {example['correct_answer']}\")\n",
    "        \n",
    "        print(f\"\\nEnsemble model answer:\")\n",
    "        print(f\"{example.get('ensemble_answer_text', 'N/A')}\")\n",
    "        print(f\"Extracted answer: {example.get('ensemble_answer', 'N/A')}\")\n",
    "        \n",
    "        print(f\"\\nBaseline model answer:\")\n",
    "        print(f\"{example.get('baseline_answer_text', 'N/A')}\")\n",
    "        print(f\"Extracted answer: {example.get('baseline_answer', 'N/A')}\")\n",
    "        \n",
    "        print(\"-\"*80)\n",
    "    \n",
    "    # Create and save visualizations\n",
    "    try:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Accuracy comparison\n",
    "        plt.subplot(2, 2, 1)\n",
    "        models = ['Ensemble', 'Baseline']\n",
    "        accuracies = [metrics['ensemble']['accuracy'], metrics['baseline']['accuracy']]\n",
    "        colors = ['green', 'blue']\n",
    "        plt.bar(models, accuracies, color=colors)\n",
    "        plt.title('Accuracy Comparison')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.ylim(0, max(max(accuracies) * 1.2, 1.0))  # Set y-limit with headroom\n",
    "        \n",
    "        # Add accuracy values on top of bars\n",
    "        for i, v in enumerate(accuracies):\n",
    "            plt.text(i, v + 0.01, f\"{v:.2%}\", ha='center', fontweight='bold')\n",
    "        \n",
    "        # Answer detection rate\n",
    "        plt.subplot(2, 2, 2)\n",
    "        detection_rates = [\n",
    "            metrics['ensemble']['answer_detection_rate'], \n",
    "            metrics['baseline']['answer_detection_rate']\n",
    "        ]\n",
    "        plt.bar(models, detection_rates, color=colors)\n",
    "        plt.title('Answer Detection Rate')\n",
    "        plt.ylabel('Rate')\n",
    "        plt.ylim(0, 1.1)  # Set y-limit with some headroom\n",
    "        \n",
    "        # Answer distribution for ensemble model (top 10)\n",
    "        plt.subplot(2, 2, 3)\n",
    "        top_ensemble_answers = list(metrics['ensemble']['answer_distribution'].keys())[:10]\n",
    "        ensemble_counts = [metrics['ensemble']['answer_distribution'][answer] for answer in top_ensemble_answers]\n",
    "        \n",
    "        # Create a horizontal bar chart for better readability of answer values\n",
    "        plt.barh(range(len(top_ensemble_answers)), ensemble_counts, color='green')\n",
    "        plt.yticks(range(len(top_ensemble_answers)), top_ensemble_answers)\n",
    "        plt.title('Top 10 Ensemble Model Answers')\n",
    "        plt.xlabel('Count')\n",
    "        \n",
    "        # Answer distribution for baseline model (top 10)\n",
    "        plt.subplot(2, 2, 4)\n",
    "        top_baseline_answers = list(metrics['baseline']['answer_distribution'].keys())[:10]\n",
    "        baseline_counts = [metrics['baseline']['answer_distribution'][answer] for answer in top_baseline_answers]\n",
    "        \n",
    "        # Create a horizontal bar chart\n",
    "        plt.barh(range(len(top_baseline_answers)), baseline_counts, color='blue')\n",
    "        plt.yticks(range(len(top_baseline_answers)), top_baseline_answers)\n",
    "        plt.title('Top 10 Baseline Model Answers')\n",
    "        plt.xlabel('Count')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save chart\n",
    "        chart_file = os.path.join(output_dir, 'gsm8k_performance_chart.png')\n",
    "        plt.savefig(chart_file)\n",
    "        print(f\"Chart saved to {chart_file}\")\n",
    "        \n",
    "        plt.close()\n",
    "        \n",
    "        # Additional visualization: Compare most common answers between models\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Combine top answers from both models\n",
    "        combined_answers = set(list(metrics['ensemble']['answer_distribution'].keys())[:5] + \n",
    "                              list(metrics['baseline']['answer_distribution'].keys())[:5])\n",
    "        \n",
    "        # Create a mapping for position on x-axis\n",
    "        x_positions = np.arange(len(combined_answers))\n",
    "        \n",
    "        # Width of each bar\n",
    "        bar_width = 0.35\n",
    "        \n",
    "        # Get counts for each model\n",
    "        ensemble_counts = [metrics['ensemble']['answer_distribution'].get(answer, 0) for answer in combined_answers]\n",
    "        baseline_counts = [metrics['baseline']['answer_distribution'].get(answer, 0) for answer in combined_answers]\n",
    "        \n",
    "        # Create grouped bars\n",
    "        plt.bar(x_positions - bar_width/2, ensemble_counts, bar_width, label='Ensemble', color='green')\n",
    "        plt.bar(x_positions + bar_width/2, baseline_counts, bar_width, label='Baseline', color='blue')\n",
    "        \n",
    "        plt.xlabel('Answer')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Most Common Answers Comparison')\n",
    "        plt.xticks(x_positions, list(combined_answers))\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save chart\n",
    "        comparison_chart_file = os.path.join(output_dir, 'gsm8k_answer_comparison.png')\n",
    "        plt.savefig(comparison_chart_file)\n",
    "        print(f\"Answer comparison chart saved to {comparison_chart_file}\")\n",
    "        \n",
    "        plt.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not create visualization: {e}\")\n",
    "    \n",
    "    # Save detailed results and error cases\n",
    "    results_file = os.path.join(output_dir, 'gsm8k_results.json')\n",
    "    with open(results_file, 'w') as f:\n",
    "        # Create a copy that includes a sample of error cases (not all to keep file size reasonable)\n",
    "        save_metrics = metrics.copy()\n",
    "        save_metrics[\"error_cases\"] = {\n",
    "            \"ensemble\": error_cases[\"ensemble\"][:10],  # Save the first 10 errors\n",
    "            \"baseline\": error_cases[\"baseline\"][:10]\n",
    "        }\n",
    "        json.dump(save_metrics, f, indent=2)\n",
    "    print(f\"Detailed results saved to {results_file}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "690c5f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 22:06:29,201 - src.model_loader - INFO - All models unloaded\n",
      "2025-05-16 22:06:29,202 - src.model_loader - INFO - ModelLoader instance deleted\n",
      "2025-05-16 22:06:29,202 - src.model_loader - INFO - Loading tokenizer for Llama-3.2-1B General (LoRA) from meta-llama/Llama-3.2-1B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 20 random samples from 1272 total examples\n",
      "Loading ensemble model on mps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 22:06:34,080 - src.model_loader - INFO - Loading model Llama-3.2-1B General (LoRA) from meta-llama/Llama-3.2-1B\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSelected 20 random samples from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(full_medqa_test)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m total examples\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.backends.mps.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m ensamble_model = \u001b[43mMultiModelWithScalarHeads\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODELS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexp1/best_model_heads.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m finetuned_model = AutoModelForCausalLM.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33maxel-datos/Llama-3.2-1B_gsm8k_full-finetuning\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m ensemble_tokenizer = ensamble_model.get_tokenizer()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI/nlp/src/architecture.py:404\u001b[39m, in \u001b[36mMultiModelWithScalarHeads.from_pretrained\u001b[39m\u001b[34m(cls, base_models_info, weights_path, head_hidden_dim, head_dropout, model_loader, device, dtype)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading ensemble model on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# Initialize the model with default (untrained) projection heads\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m model = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_models\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_models_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_hidden_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_hidden_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_dropout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[38;5;66;03m# Load the saved projection head weights\u001b[39;00m\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(weights_path):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI/nlp/src/architecture.py:82\u001b[39m, in \u001b[36mMultiModelWithScalarHeads.__init__\u001b[39m\u001b[34m(self, base_models, head_hidden_dim, head_dropout, model_loader, device, dtype)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Load all models \u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_info \u001b[38;5;129;01min\u001b[39;00m base_models:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     model, tokenizer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     model_id = model_info.id\n\u001b[32m     84\u001b[39m     \u001b[38;5;28mself\u001b[39m.base_models[model_id] = model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI/nlp/src/model_loader.py:134\u001b[39m, in \u001b[36mModelLoader.get_model\u001b[39m\u001b[34m(self, model_info)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Convenience method to load a model and return it, or get from cache if already loaded.\"\"\"\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m model_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mModel info cannot be None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_info\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI/nlp/src/model_loader.py:70\u001b[39m, in \u001b[36mModelLoader.load_model\u001b[39m\u001b[34m(self, model_info)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     68\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_info.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_info.path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#add args+ if needed\u001b[39;49;00m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mFailed to load model\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m     \u001b[38;5;66;03m# Move model to appropriate device\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI/nlp/myvenv/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI/nlp/myvenv/lib/python3.13/site-packages/transformers/modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI/nlp/myvenv/lib/python3.13/site-packages/transformers/modeling_utils.py:4260\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4251\u001b[39m     gguf_file\n\u001b[32m   4252\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4253\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map.values()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[32m   4254\u001b[39m ):\n\u001b[32m   4255\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   4256\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mloaded from GGUF files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4260\u001b[39m checkpoint_files, sharded_metadata = \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4262\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4267\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4273\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4276\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4278\u001b[39m is_sharded = sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4279\u001b[39m is_quantized = hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI/nlp/myvenv/lib/python3.13/site-packages/transformers/modeling_utils.py:997\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash)\u001b[39m\n\u001b[32m    982\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    983\u001b[39m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[32m    984\u001b[39m     cached_file_kwargs = {\n\u001b[32m    985\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcache_dir\u001b[39m\u001b[33m\"\u001b[39m: cache_dir,\n\u001b[32m    986\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mforce_download\u001b[39m\u001b[33m\"\u001b[39m: force_download,\n\u001b[32m   (...)\u001b[39m\u001b[32m    995\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m: commit_hash,\n\u001b[32m    996\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m     resolved_archive_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    999\u001b[39m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[32m   1000\u001b[39m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[32m   1001\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename == _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[32m   1002\u001b[39m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI/nlp/myvenv/lib/python3.13/site-packages/transformers/utils/hub.py:266\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    209\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    210\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    211\u001b[39m     **kwargs,\n\u001b[32m    212\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    213\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    215\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    264\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI/nlp/myvenv/lib/python3.13/site-packages/transformers/utils/hub.py:424\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    422\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    423\u001b[39m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    439\u001b[39m         snapshot_download(\n\u001b[32m    440\u001b[39m             path_or_repo_id,\n\u001b[32m    441\u001b[39m             allow_patterns=full_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    450\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    451\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI/nlp/myvenv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI/nlp/myvenv/lib/python3.13/site-packages/huggingface_hub/file_download.py:1008\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    989\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    990\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1005\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1006\u001b[39m     )\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI/nlp/myvenv/lib/python3.13/site-packages/huggingface_hub/file_download.py:1159\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1157\u001b[39m Path(lock_path).parent.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1158\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[32m-> \u001b[39m\u001b[32m1159\u001b[39m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.incomplete\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1171\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(pointer_path):\n\u001b[32m   1172\u001b[39m         _create_symlink(blob_path, pointer_path, new_blob=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI/nlp/myvenv/lib/python3.13/site-packages/huggingface_hub/file_download.py:1723\u001b[39m, in \u001b[36m_download_to_tmp_and_move\u001b[39m\u001b[34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[39m\n\u001b[32m   1716\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1717\u001b[39m             logger.warning(\n\u001b[32m   1718\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mXet Storage is enabled for this repo, but the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhf_xet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m package is not installed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1719\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFalling back to regular HTTP download. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1720\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1721\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m1723\u001b[39m         \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1724\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1725\u001b[39m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1726\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1727\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1728\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1729\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1730\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1732\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1733\u001b[39m _chmod_and_move(incomplete_path, destination_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI/nlp/myvenv/lib/python3.13/site-packages/huggingface_hub/file_download.py:494\u001b[39m, in \u001b[36mhttp_get\u001b[39m\u001b[34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[39m\n\u001b[32m    492\u001b[39m new_resume_size = resume_size\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n\u001b[32m    496\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI/nlp/myvenv/lib/python3.13/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI/nlp/myvenv/lib/python3.13/site-packages/urllib3/response.py:1066\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1064\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1068\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[32m   1069\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI/nlp/myvenv/lib/python3.13/site-packages/urllib3/response.py:955\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt, decode_content, cache_content)\u001b[39m\n\u001b[32m    952\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) >= amt:\n\u001b[32m    953\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoded_buffer.get(amt)\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    957\u001b[39m flush_decoder = amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[32m    959\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI/nlp/myvenv/lib/python3.13/site-packages/urllib3/response.py:879\u001b[39m, in \u001b[36mHTTPResponse._raw_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    876\u001b[39m fp_closed = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._fp, \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    878\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._error_catcher():\n\u001b[32m--> \u001b[39m\u001b[32m879\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[32m    882\u001b[39m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    887\u001b[39m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[32m    888\u001b[39m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[32m    889\u001b[39m         \u001b[38;5;28mself\u001b[39m._fp.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI/nlp/myvenv/lib/python3.13/site-packages/urllib3/response.py:862\u001b[39m, in \u001b[36mHTTPResponse._fp_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    859\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1()\n\u001b[32m    860\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    861\u001b[39m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m862\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:479\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    478\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import random\n",
    "from src.architecture import MultiModelWithScalarHeads\n",
    "from src.models import MODELS\n",
    "from tqdm import tqdm\n",
    "\n",
    "full_medqa_test = load_dataset(\"cola13/medqa_formatted\", split=\"test\")\n",
    "\n",
    "# Create a random subset of 20 samples\n",
    "random_indices = random.sample(range(len(full_medqa_test)), 1)\n",
    "medqa_test = full_medqa_test.select(random_indices)\n",
    "\n",
    "print(f\"Selected 20 random samples from {len(full_medqa_test)} total examples\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "ensamble_model = MultiModelWithScalarHeads.from_pretrained(MODELS, \"exp1/best_model_heads.pt\")\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(\"axel-datos/Llama-3.2-1B_gsm8k_full-finetuning\")\n",
    "\n",
    "ensemble_tokenizer = ensamble_model.get_tokenizer()\n",
    "finetuned_tokenizer = AutoTokenizer.from_pretrained(\"axel-datos/Llama-3.2-1B_gsm8k_full-finetuning\")\n",
    "\n",
    "test_medqa(ensamble_model, finetuned_model, ensemble_tokenizer, finetuned_tokenizer, medqa_test, \"medqa_results\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57682f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 20 random samples from 1319 total examples\n",
      "Testing on 20 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing models:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ensemble model on example 0...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "full_gsm8k_test = load_dataset(\"cola13/gsm8k_formatted\", split=\"test\")\n",
    "\n",
    "# Create a random subset of 20 samples\n",
    "random_indices = random.sample(range(len(full_medqa_test)), 20)\n",
    "gsm8k_test = full_gsm8k_test.select(random_indices)\n",
    "\n",
    "print(f\"Selected 20 random samples from {len(full_gsm8k_test)} total examples\")\n",
    "\n",
    "test_gsm8k(general_model, other_model, general_tokenizer, other_tokenizer, gsm8k_test, \"gsm8k_results\", device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
